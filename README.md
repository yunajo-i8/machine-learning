### **1. 지도학습(Supervised Learning)이란?**

- **입력 데이터와 정답(label)이 함께 주어진 상태**에서 모델이 학습하는 방법
- 정답이 명확한 문제에 적합
- **학습과정**
    1. 입력 데이터 & 정답 제공
    2. 모델이 입력을 처리해 예측값 생성
    3. 예측값과 정답 간 오차 계산 → 비용함수
    4. 비용함수 최소화되도록 파라미터 조정 (최적화) 반복
    5. 새로운 입력에 대해 정확한 예측 수행

### 2. 회귀

연속적인 숫자를 예측하는 문제이다(집값예측, 온도예측 등)

- 선형회귀 : 입력과 출력 사이의 관계를 직선으로 표현하는 기법(데이터에 가장 잘 맞는 직선을 찾음) h(x) = wx + b
- MSE(평균제곱오차) : 선형회귀에서 구한 직선(예측값)이 실제 데이터와 얼마나 차이가 나는지 계산하는 오차 지표

# **3. 분류**

어떤 클래스(범위)에 속하는 지를 찾는 문제이다. 
(고양이/강아지, 정상/불량 등)

- 주요 분류 알고리즘

1)_Logistic Regression : 확률을 이용해서 데이터 분류

(예시) 0.5이상이면 참, 미만이면 거짓

2개 클래스로 나뉠때 많이 쓰임

2)_Naive Bayes(나이브 베이즈) : 베이즈 정리(확률 수학 법칙)를 이용해서 분류하는 방법. 

각 특징들이 서로 영향을 미치지 않는다고 가정하고 가장 가능성 높은 클래스를 선택하는 방식(스팸인지 아닌지)

3)_SVM(서포트벡터머신) : 클래스를 최대한 멀리 떨어뜨리는 경계선 찾는 방법. 

4)_kNN(k-최근접이웃) : 가장 가까운 이웃(데이터)을 참고해서 어느 클래스에 속하는지 결정

5)_Decision Tree(결정 트리) : 질문을 단계적으로 나눠서 분류(키가 170이상인가, 나이가 20이상인가 - - -, 질문을 나무 가지처럼 계속 나눠가면서 분류함)

# **4. 과소적합 / 과대적합**
- 과소적합 : 학습 데이터의 중요한 패턴과 구조를 충분히 학습하지 못한 상태.(편향 높음) 예측 성능이 낮음. 해결방안으로 더 많은 학습데이터 확보하거나 모델 복잡도를 늘려야 함.
- 과대적합 : 학습 데이터에 지나치게 적합해서 학습데이터 노이즈나 미세한 변동까지 학습해버리는 현상으로 과도한 복잡성으로 일반화 성능이 떨어짐.(분산 높음) 해결방안으로 정규화, 교차검증, 조기종료 기법을 사용함
- 편향과 분산 두 가지 균형을 맞춰야 예측 성능을 높일 수 있다(Bias-Variance Tradeoff)

# **5.모델 선택**

- 데이터 분리

모델이 제대로 배우고 평가받으려면 데이터를 세부분으로 나눈다.

1)_Train(학습용 데이터) : 모델을 가르침

2)_Validation(검증용 데이터) : 어떤 모델이 더 좋은지 비교

3)_Test(테스트용 데이터) : 모델 성능을 평가하는 데 사용

- k-fold 교차검증

데이터가 적을때 검증용 데이터와 학습용 데이터를 나눈 후 검증용 데이터를 바꾸면서 반복함.(데이터가 적어도 많은 데이터를 학습과 검증에 활용할수있음)

# **6. 규제**

과대적합을 막기 위한 기법

- L2(Ridge) : 가중치가 너무 커지지 않도록 제약을 검(패널티 주는 방식) 가중치 줄임
- L1(Lasso) : 일부 가중치(중요하지않은 값)을 0으로 만들어 버림
- 딥러닝에서의 규제

1)_Dropout : 훈련할때 신경망의 일부 뉴런을 임의로 꺼버려서 의존하지 않게함 → 일반화 능력 좋아짐

2)_Early Stopping : 모델 학습시키다가 검증용 데이터에서 성능이 더 이상 좋아지지 않고 멈추거나 나빠지면 훈련을 멈춰버림.

3)_Data Augmentation : 데이터가 부족할 때 인위적으로 데이터를 늘려주는 기법

4)_Ensemble(앙상블) : 여러 모델의 결과를 모아서 결론을 내림

# **7. 차원축소(PCA)**

특징이 너무 많을 때 중요한 정보만 남김.

데이터 패턴을 시각화 하기에 좋고, 노이즈 제거되고, 연산 효율이 높아지는 장점이 있다.

(과정) 공분산 행렬 계산 → 고유값과 고유벡터 계산 → 분산이 큰 방향 선택 → 차원축소

# **8. 앙상블(Ensemble)**

여러 개의 모델을 이용해 결정을 하는 방법

1) Voting(보팅) : 여러 모델들이 내놓은 결과에 투표해서 최종결정함(다수결)

2) Bagging(배깅) : 데이터를 여러번 샘플링해서 다양한 모델들을 독립적으로 학습시키고 결과들의 평균을 내거나 투표로 합쳐서 성능을 안정화해

3) Boosting(부스팅) : 첫 모델이 잘 못 옟측한 데이터에 더 집중해 다음 모델이 학습함. 여러 모델이 뭉쳐서 성능을 높이는 방법 

4) Random Forest(랜덤 포레스트) : 여러 개의 결정트리(Decision Tree)를 만들고, 각각 조금씩 다르게 학습해서 결과를 합치는 대표적인 배깅 방법

# **9. sklearn 기반 실습 (Python)**

- 사용 라이브러리 : numpy (행렬 연산), pandas (전처리), matplotlib (시각화), scikit-learn
- 모델 종류
    - LinearRegression
    - LogisticRegression
    - KNN
    - SVM
    - DecisionTree
    - RandomForest
    - PCA
    - KMeans
