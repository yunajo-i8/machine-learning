### **1. 지도학습(Supervised Learning)이란?**

- **입력 데이터와 정답(label)이 함께 주어진 상태**에서 모델이 학습하는 방법
- 정답이 명확한 문제에 적합
- **학습과정**
    1. 입력 데이터 & 정답 제공
    2. 모델이 입력을 처리해 예측값 생성
    3. 예측값과 정답 간 오차 계산 → 비용함수
    4. 비용함수 최소화되도록 파라미터 조정 (최적화) 반복
    5. 새로운 입력에 대해 정확한 예측 수행




### **2. 회귀**

- 연속적인 숫자를 예측하는 문제(예: 집값예측, 온도예측 등)

- 선형회귀 : 입력과 출력 사이의 관계를 직선으로 표현하는 기법
            (데이터에 가장 잘 맞는 직선을 찾음)
    - h(x) = wx + b
- MSE(평균제곱오차) : 선형회귀에서 구한 직선(예측값)이 실제 데이터와 얼마나 차이가 나는지 계산하는 오차 지표



### **3. 분류**

어떤 클래스(범위)에 속하는 지를 찾는 문제이다. 
(고양이/강아지, 정상/불량 등)

- 주요 분류 알고리즘

      1)_Logistic Regression : 확률을 이용해서 데이터 분류
    - (예시) 0.5이상이면 참, 미만이면 거짓

    - 2개 클래스로 나뉠때 많이 쓰임

    ```python
    2) Naive Bayes(나이브 베이즈) : 베이즈 정리(확률 수학 법칙)를 이용해서 분류하는 방법. 
    ```

    - 각 특징들이 서로 영향을 미치지 않는다고 가정하고 가장 가능성 높은 클래스를 선택하는 방식(스팸인지 아닌지)

    ```python
    3) SVM(서포트벡터머신) : 클래스를 최대한 멀리 떨어뜨리는 경계선 찾는 방법.
    ``` 

    ```python
    4) kNN(k-최근접이웃) : 가장 가까운 이웃(데이터)을 참고해서 어느 클래스에 속하는지 결정
    ``` 

    ```python
    5) Decision Tree(결정 트리) : 질문을 단계적으로 나눠서 분류(키가 170이상인가, 나이가 20이상인가 - - -, 질문을 나무 가지처럼 계속 나눠가면서 분류함)
    ```
    
### 4. 과소적합 / 과대적합

**▪ 과소적합(Underfitting)**

- 데이터의 중요한 패턴을 충분히 학습하지 못한 상태
- 모델이 너무 단순해서 학습·테스트 모두에서 성능이 낮음
- 특징: 편향(Bias) ↑, 분산(Variance) ↓
- 해결 방법
    - 더 많은 학습 데이터 사용
    - 모델 복잡도 증가(특성 추가, 더 깊은 모델 사용 등)
    - 학습 시간 증가(에폭 수 증가 등)

---

**▪ 과대적합(Overfitting)**

- 학습 데이터에 지나치게 맞춰져 노이즈까지 학습한 상태
- 학습 데이터 성능은 높지만 새로운 데이터에서 성능 저하
- 특징: 편향(Bias) ↓, 분산(Variance) ↑
- 해결 방법
    - 정규화(L1, L2 등) 적용
    - 교차검증(Cross Validation) 사용
    - 조기 종료(Early Stopping)
    - Dropout, 데이터 증강 등 활용

---

**▪ 편향–분산 트레이드오프(Bias–Variance Tradeoff)**

- 편향과 분산은 서로 반비례 관계에 있음
- 모델이 단순하면 → 편향 ↑, 분산 ↓ (과소적합 발생)
- 모델이 복잡하면 → 편향 ↓, 분산 ↑ (과대적합 발생)
- 두 요소의 균형을 맞추는 것이 핵심이며, 이를 통해 일반화 성능(새로운 데이터 예측 능력)이 가장 높아짐.
## 5. 모델 선택과 데이터 분리

모델이 제대로 학습되고 공정하게 평가되기 위해 데이터를 세 부분으로 나눈다.

- **Train(학습용 데이터)**
    
    → 모델을 학습시키는 데 사용
    
- **Validation(검증용 데이터)**
    
    → 모델 비교 및 하이퍼파라미터 튜닝에 사용
    
- **Test(테스트용 데이터)**
    
    → 최종 성능 평가에 사용 (학습 과정에 사용하지 않음)
    

---

## 6. k-fold 교차검증

데이터가 부족할 때 사용하는 검증 방법

- 데이터를 k개로 나눔
- 그중 1개를 검증용, 나머지를 학습용으로 사용
- 이 과정을 k번 반복
- 평균 성능으로 모델 평가

👉 적은 데이터로도 학습과 검증을 반복 수행할 수 있음.

---

## 7. 규제(Regularization)

과대적합을 방지하기 위한 기법

### ▪ L2 규제 (Ridge)

- 가중치가 너무 커지지 않도록 패널티 부여
- 전체 가중치를 작게 만드는 효과

### ▪ L1 규제 (Lasso)

- 중요하지 않은 가중치를 0으로 만듦
- 특성 선택(Feature Selection) 효과

---

## 8. 딥러닝에서의 규제 기법

- **Dropout**
    
    → 일부 뉴런을 임의로 꺼서 특정 뉴런에 과도하게 의존하지 않도록 함
    
    → 일반화 성능 향상
    
- **Early Stopping**
    
    → 검증 데이터 성능이 더 이상 좋아지지 않으면 학습 중단
    
- **Data Augmentation**
    
    → 데이터가 부족할 때 인위적으로 데이터 생성
    
- **Ensemble**
    
    → 여러 모델 결과를 결합하여 성능 향상
    

---

## 9. 차원 축소 (PCA)

특성이 많을 때 중요한 정보만 남기고 차원을 줄이는 방법

### 장점

- 데이터 시각화 용이
- 노이즈 제거
- 연산 효율 증가

### 과정

1. 공분산 행렬 계산
2. 고유값·고유벡터 계산
3. 분산이 큰 방향 선택
4. 차원 축소 수행

---

## 10. 앙상블 (Ensemble)

여러 모델을 결합하여 더 좋은 성능을 만드는 방법

- **Voting**
    
    → 여러 모델의 예측 결과를 투표로 결정
    
- **Bagging**
    
    → 데이터를 여러 번 샘플링하여 독립적으로 모델 학습 후 평균/투표
    
- **Boosting**
    
    → 이전 모델이 틀린 데이터에 집중하여 다음 모델이 학습
    
- **Random Forest**
    
    → 여러 결정트리를 생성해 결과를 합치는 대표적인 Bagging 기법
    

---

## 11. sklearn 기반 실습 정리

### 사용 라이브러리

- **numpy** : 행렬 연산
- **pandas** : 데이터 전처리
- **matplotlib** : 시각화
- **scikit-learn** : 머신러닝 모델 구현

### 사용 모델

- LinearRegression
- LogisticRegression
- KNN
- SVM
- DecisionTree
- RandomForest
- PCA
- KMeans
