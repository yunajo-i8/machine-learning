## **1. 지도학습(Supervised Learning)이란?**

- **입력 데이터와 정답(label)이 함께 주어진 상태**에서 모델이 학습하는 방법
- 정답이 명확한 문제에 적합
- ## **학습과정**
    1. 입력 데이터 & 정답 제공
    2. 모델이 입력을 처리해 예측값 생성
    3. 예측값과 정답 간 오차 계산 → 비용함수
    4. 비용함수 최소화되도록 파라미터 조정 (최적화) 반복
    5. 새로운 입력에 대해 정확한 예측 수행
---

## 2. 회귀(Regression)

연속적인 값을 예측하는 문제

예: 집값 예측, 온도 예측, 매출 예측

- **선형회귀(Linear Regression)**
    
    입력과 출력 사이의 관계를 직선으로 표현
    
    → 데이터에 가장 잘 맞는 직선 탐색
    
    → h(x)=wx+bh(x) = wx + bh(x)=wx+b
    
- **MSE(평균제곱오차)**
    
    예측값과 실제값 차이를 제곱하여 평균낸 오차 지표
    
    → 값이 작을수록 모델 성능이 좋음
    

---

## 3. 분류(Classification)

데이터가 어떤 클래스에 속하는지 판단하는 문제

예: 고양이/강아지, 정상/불량, 스팸/일반 메일

### 주요 분류 알고리즘

- **Logistic Regression**
    
    확률 기반으로 데이터를 분류
    
    (예: 확률 0.5 이상 → 특정 클래스)
    
- **Naive Bayes**
    
    베이즈 정리를 기반으로 분류
    
    각 특징이 서로 독립이라고 가정
    
    → 스팸 분류 등에 활용
    
- **SVM (Support Vector Machine)**
    
    클래스 간 경계를 최대한 넓게 만드는 초평면 탐색
    
- **kNN (k-최근접 이웃)**
    
    가장 가까운 데이터 k개를 참고하여 분류
    
- **Decision Tree (결정 트리)**
    
    조건을 단계적으로 나누며 분류
    
    (예: 키 > 170? → 나이 > 20? → …)
    

---

## 4. 과소적합 / 과대적합

### ▪ 과소적합(Underfitting)

- 데이터의 중요한 패턴을 충분히 학습하지 못한 상태
- 학습·테스트 모두에서 성능이 낮음
- 특징: 편향(Bias) ↑
- 해결 방법
    - 더 많은 데이터 확보
    - 모델 복잡도 증가

### ▪ 과대적합(Overfitting)

- 학습 데이터에 지나치게 맞춰져 노이즈까지 학습한 상태
- 학습 데이터 성능은 높지만 새로운 데이터에서 성능 저하
- 특징: 분산(Variance) ↑
- 해결 방법
    - 정규화
    - 교차검증
    - 조기 종료(Early Stopping)

---

## 5. 편향–분산 트레이드오프 (Bias–Variance Tradeoff)

- 모델이 단순하면 → 편향 ↑, 분산 ↓ → 과소적합 발생
- 모델이 복잡하면 → 편향 ↓, 분산 ↑ → 과대적합 발생
- 두 요소의 균형을 맞추는 것이 중요하며
    
    이를 통해 새로운 데이터에 대한 예측 성능(일반화 성능)이 가장 높아진다.

## 5. 모델 선택과 데이터 분리

모델이 제대로 학습되고 공정하게 평가되기 위해 데이터를 세 부분으로 나눈다.

- **Train(학습용 데이터)**
    
    → 모델을 학습시키는 데 사용
    
- **Validation(검증용 데이터)**
    
    → 모델 비교 및 하이퍼파라미터 튜닝에 사용
    
- **Test(테스트용 데이터)**
    
    → 최종 성능 평가에 사용 (학습 과정에 사용하지 않음)
    

---

## 6. k-fold 교차검증

데이터가 부족할 때 사용하는 검증 방법

- 데이터를 k개로 나눔
- 그중 1개를 검증용, 나머지를 학습용으로 사용
- 이 과정을 k번 반복
- 평균 성능으로 모델 평가

👉 적은 데이터로도 학습과 검증을 반복 수행할 수 있음.

---

## 7. 규제(Regularization)

과대적합을 방지하기 위한 기법

### ▪ L2 규제 (Ridge)

- 가중치가 너무 커지지 않도록 패널티 부여
- 전체 가중치를 작게 만드는 효과

### ▪ L1 규제 (Lasso)

- 중요하지 않은 가중치를 0으로 만듦
- 특성 선택(Feature Selection) 효과

---

## 8. 딥러닝에서의 규제 기법

- **Dropout**
    
    → 일부 뉴런을 임의로 꺼서 특정 뉴런에 과도하게 의존하지 않도록 함
    
    → 일반화 성능 향상
    
- **Early Stopping**
    
    → 검증 데이터 성능이 더 이상 좋아지지 않으면 학습 중단
    
- **Data Augmentation**
    
    → 데이터가 부족할 때 인위적으로 데이터 생성
    
- **Ensemble**
    
    → 여러 모델 결과를 결합하여 성능 향상
    

---

## 9. 차원 축소 (PCA)

특성이 많을 때 중요한 정보만 남기고 차원을 줄이는 방법

### 장점

- 데이터 시각화 용이
- 노이즈 제거
- 연산 효율 증가

### 과정

1. 공분산 행렬 계산
2. 고유값·고유벡터 계산
3. 분산이 큰 방향 선택
4. 차원 축소 수행

---

## 10. 앙상블 (Ensemble)

여러 모델을 결합하여 더 좋은 성능을 만드는 방법

- **Voting**
    
    → 여러 모델의 예측 결과를 투표로 결정
    
- **Bagging**
    
    → 데이터를 여러 번 샘플링하여 독립적으로 모델 학습 후 평균/투표
    
- **Boosting**
    
    → 이전 모델이 틀린 데이터에 집중하여 다음 모델이 학습
    
- **Random Forest**
    
    → 여러 결정트리를 생성해 결과를 합치는 대표적인 Bagging 기법
    

---

## 11. sklearn 기반 실습 정리

### 사용 라이브러리

- **numpy** : 행렬 연산
- **pandas** : 데이터 전처리
- **matplotlib** : 시각화
- **scikit-learn** : 머신러닝 모델 구현

### 사용 모델

- LinearRegression
- LogisticRegression
- KNN
- SVM
- DecisionTree
- RandomForest
- PCA
- KMeans
