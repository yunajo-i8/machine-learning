### **지도학습(Supervised Learning)이란?**

- **입력 데이터와 정답(label)이 함께 주어진 상태**에서 모델이 학습하는 방법
- 정답이 명확한 문제에 적합
- **학습과정**
    1. 입력 데이터 & 정답 제공
    2. 모델이 입력을 처리해 예측값 생성
    3. 예측값과 정답 간 오차 계산 → 비용함수
    4. 비용함수 최소화되도록 파라미터 조정 (최적화) 반복
    5. 새로운 입력에 대해 정확한 예측 수행

회귀(Regression)

연속적인 숫자를 예측하는 문제야 (예: 집값, 온도 예측 등)
대표적인 방법은 선형회귀로, 입력과 출력 사이 관계를 직선으로 표현해
수식: h(x)=wx+b
모델 평가 시 평균제곱오차(MSE)를 활용해서 예측값과 실제값 차이를 계산해

1. 분류

어떤 클래스(범위)에 속하는 지를 찾는 문제이다. 
(고양이/강아지, 정상/불량 등)

- 주요 분류 알고리즘

1)_Logistic Regression : 확률을 이용해서 데이터 분류

(예시) 0.5이상이면 참, 미만이면 거짓

2개 클래스로 나뉠때 많이 쓰임

2)_Naive Bayes(나이브 베이즈) : 베이즈 정리(확률 수학 법칙)를 이용해서 분류하는 방법. 

각 특징들이 서로 영향을 미치지 않는다고 가정하고 가장 가능성 높은 클래스를 선택하는 방식(스팸인지 아닌지)

3)_SVM(서포트벡터머신) : 클래스를 최대한 멀리 떨어뜨리는 경계선 찾는 방법. 

4)_kNN(k-최근접이웃) : 가장 가까운 이웃(데이터)을 참고해서 어느 클래스에 속하는지 결정

5)_Decision Tree(결정 트리) : 질문을 단계적으로 나눠서 분류(키가 170이상인가, 나이가 20이상인가 - - -, 질문을 나무 가지처럼 계속 나눠가면서 분류함)

3. 과소적합과 과대적합
구분	의미	특징	해결법 예시
과소적합	패턴을 충분히 학습하지 못한 상태	편향(Bias) 높음	복잡도 증가, 데이터 추가
과대적합	데이터 노이즈까지 학습해 너무 맞춤	분산(Variance) 높음	정규화, 교차검증, 조기 종료

1. 모델 선택
- 데이터 분리

모델이 제대로 배우고 평가받으려면 데이터를 세부분으로 나눈다.

1)_Train(학습용 데이터) : 모델을 가르침

2)_Validation(검증용 데이터) : 어떤 모델이 더 좋은지 비교

3)_Test(테스트용 데이터) : 모델 성능을 평가하는 데 사용

- k-fold 교차검증

데이터가 적을때 검증용 데이터와 학습용 데이터를 나눈 후 검증용 데이터를 바꾸면서 반복함.(데이터가 적어도 많은 데이터를 학습과 검증에 활용할수있음)

1. 규제

과대적합을 막기 위한 기법

- L2(Ridge) : 가중치가 너무 커지지 않도록 제약을 검(패널티 주는 방식) 가중치 줄임
- L1(Lasso) : 일부 가중치(중요하지않은 값)을 0으로 만들어 버림
- 딥러닝에서의 규제

1)_Dropout : 훈련할때 신경망의 일부 뉴런을 임의로 꺼버려서 의존하지 않게함 → 일반화 능력 좋아짐

2)_Early Stopping : 모델 학습시키다가 검증용 데이터에서 성능이 더 이상 좋아지지 않고 멈추거나 나빠지면 훈련을 멈춰버림.

3)_Data Augmentation : 데이터가 부족할 때 인위적으로 데이터를 늘려주는 기법

4)_Ensemble(앙상블) : 여러 모델의 결과를 모아서 결론을 내림

1. 차원축소(PCA)

특징이 너무 많을 때 중요한 정보만 남김.

데이터 패턴을 시각화 하기에 좋고, 노이즈 제거되고, 연산 효율이 높아지는 장점이 있다.

(과정) 공분산 행렬 계산 → 고유값과 고유벡터 계산 → 분산이 큰 방향 선택 → 차원축소

1. 앙상블(Ensemble)

여러 개의 모델을 이용해 결정을 하는 방법

1)_Voting(보팅) : 여러 모델들이 내놓은 결과에 투표해서 최종결정함(다수결)

2)_Bagging(배깅) : 데이터를 여러번 샘플링해서 다양한 모델들을 독립적으로 학습시키고 결과들의 평균을 내거나 투표로 합쳐서 성능을 안정화해

3)_Boosting(부스팅) : 첫 모델이 잘 못 옟측한 데이터에 더 집중해 다음 모델이 학습함. 여러 모델이 뭉쳐서 성능을 높이는 방법 

4)_Random Forest(랜덤 포레스트) : 여러 개의 결정트리(Decision Tree)를 만들고, 각각 조금씩 다르게 학습해서 결과를 합치는 대표적인 배깅 방법

- sklearn 기반 코드 실습 내용(python)

### ✔ 사용 라이브러리

- numpy(행렬 연산), pandas(전처리), matplotlib(시각화)
- scikit-learn (sklearn)

### ✔ 모델 종류

- LinearRegression
- LogisticRegression
- KNN
- SVM
- DecisionTree
- RandomForest
- PCA
- KMeans
